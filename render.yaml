# Render Deployment Configuration for InferenceMD
# Defines the services needed to run the application on Render.
# Documentation: https://render.com/docs/blueprint-spec

databases:
  # Database service for the FastAPI backend
  - name: inferencemd-db
    # region: frankfurt # Optional: Choose a region close to you/your users
    plan: free # Or choose a paid plan for production needs
    postgresMajorVersion: 14 # Specify PostgreSQL version
    ipAllowList: [] # Allows connections from Render services by default

services:
  # Backend Web Service (FastAPI)
  # Runs the Python backend application using the Dockerfile.
  - type: web
    name: inferencemd-backend
    # region: frankfurt # Optional: Match the database region
    runtime: docker # Specifies that we are using a Dockerfile
    plan: free
    dockerfilePath: ./backend/Dockerfile # Path to the Dockerfile relative to the repo root
    dockerContext: ./backend # Build context for Docker
    healthCheckPath: /api/health # Optional: Add a health check endpoint to FastAPI
    envVars:
      # Database and Core Settings
      - key: DATABASE_URL
        fromDatabase:
          name: inferencemd-db # Name of the database service defined above
          property: connectionString # Get the connection string
      - key: SECRET_KEY
        generateValue: true # Let Render generate a secure secret key
      - key: PYTHON_VERSION # Specify Python version used in Dockerfile
        value: "3.10.13" # Match the version used in python:3.10-slim or update Dockerfile
      - key: CORS_ORIGINS # Allow frontend origin (Update with your frontend URL)
        # Format as a JSON string list to ensure correct parsing by Pydantic Settings validator
        value: '["https://inferencemd-frontend.onrender.com"]'
      
      # LLM Provider Configuration
      # Set this to one of: "azure", "openai", "gemini", "deepseek"
      - key: LLM_PROVIDER
        value: "gemini"
      
      # Azure OpenAI Settings
      # - key: AZURE_OPENAI_API_VERSION
      #   value: "2023-05-15"
      # These need to be set in Render UI:
      # - AZURE_OPENAI_API_KEY
      # - AZURE_OPENAI_API_BASE
      # - AZURE_OPENAI_DEPLOYMENT_NAME
      
      # OpenAI Settings (when LLM_PROVIDER="openai")
      # - key: OPENAI_API_KEY
      # - key: LLM_MODEL_NAME (e.g., "gpt-4", "gpt-3.5-turbo")
      
      # Google Gemini Settings (when LLM_PROVIDER="gemini")
      - key: GOOGLE_API_KEY
        value: AIzaSyCBDfjaUW19EffSNoaBlRIZt3zx-FonLyQ
      - key: LLM_MODEL_NAME
        value: gemini-2.5-pro-preview-03-25
      
      # DeepSeek Settings (when LLM_PROVIDER="deepseek")
      # - key: DEEPSEEK_API_KEY
      # - key: DEEPSEEK_API_BASE (default: "https://api.deepseek.com/v1")
      # - key: LLM_MODEL_NAME (e.g., "deepseek-chat")
      
      # Common LLM Parameters (apply to all providers)
      - key: LLM_TEMPERATURE
        value: "0.5"
      - key: LLM_MAX_TOKENS
        value: "4096"
      - key: LLM_TIMEOUT
        value: "120"
      - key: LLM_MAX_RETRIES
        value: "2"
      
      # File Storage Settings
      - key: REPORTS_DIR # Point to the mounted disk path
        value: /tmp/reports
      - key: NOTES_DIR # Point to the mounted disk path
        value: /tmp/notes
      - key: STATIC_DIR
        value: "/app/static"
    # Disk configuration removed as it's not supported in free tier
    buildCommand: |
      pip install -r requirements.txt
      alembic upgrade head
    startCommand: gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.main:app

  # Frontend Static Site (React)
  # Builds and serves the static React application.
  - type: web
    name: inferencemd-frontend
    # region: frankfurt # Optional: Match other services
    runtime: static # Required for static sites
    buildCommand: npm install && npm run build # Commands to build the site
    staticPublishPath: build # Directory containing the built static assets (relative to rootDir)
    rootDir: frontend # Root directory for the frontend code (relative to repo root)
    envVars:
      - key: REACT_APP_API_BASE_URL # Pass backend URL to frontend
        fromService:
          type: web # Service type
          name: inferencemd-backend # Name of the backend service
          property: host # Use valid property
    routes:
      - type: rewrite
        source: /*
        destination: /index.html

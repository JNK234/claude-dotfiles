# Render Deployment Configuration for InferenceMD
# Defines the services needed to run the application on Render.
# Documentation: https://render.com/docs/blueprint-spec

databases:
  # Database service for the FastAPI backend
  - name: inferencemd-db
    # region: frankfurt # Optional: Choose a region close to you/your users
    plan: free # Or choose a paid plan for production needs
    postgresMajorVersion: 14 # Specify PostgreSQL version
    ipAllowList: [] # Allows connections from Render services by default
    # Add SSL mode for better security
    sslmode: require

services:
  # Backend Web Service (FastAPI)
  # Runs the Python backend application using Docker.
  - type: web
    name: inferencemd-backend
    # Using Docker runtime
    runtime: docker
    plan: free
    # Docker specific settings
    dockerfilePath: ./backend/Dockerfile # Path to the Dockerfile relative to repo root
    dockerContext: . # Build context directory is the repository root
    healthCheckPath: /health
    autoDeploy: true
    envVars:
      # Database and Core Settings
      - key: DATABASE_URL
        fromDatabase:
          name: inferencemd-db
          property: connectionString
      - key: SECRET_KEY
        generateValue: true
      # PYTHON_VERSION is handled by pythonVersion field above
      # PYTHONPATH is not typically needed when running from the correct directory
      # CORS_ORIGINS will be constructed in config.py using RENDER_FRONTEND_URL
      - key: RENDER_FRONTEND_URL
        value: https://inferencemd-frontend.onrender.com # Replace with your actual frontend URL
      
      # LLM Provider Configuration
      # Set this to one of: "azure", "openai", "gemini", "deepseek"
      - key: LLM_PROVIDER
        value: "gemini"
      
      # Google Gemini Settings (when LLM_PROVIDER="gemini")
      # IMPORTANT: Add your API key in the Render dashboard's environment variables
      - key: GOOGLE_API_KEY
        sync: false # This indicates that the value should be set in the Render dashboard
      - key: LLM_MODEL_NAME
        value: gemini-2.5-pro-preview-03-25 # Example: Use a specific model version
      
      # Common LLM Parameters (apply to all providers)
      - key: LLM_TEMPERATURE
        value: "0.5"
        
      - key: LLM_MAX_TOKENS
        value: "4096"
      - key: LLM_TIMEOUT
        value: "120"
      - key: LLM_MAX_RETRIES
        value: "2"
      
      # File Storage Settings - Removed, defaults in config.py will be used
      # (e.g., 'reports', 'notes', 'static' relative to backend dir)
      # If persistent storage is needed, add a Render Disk and set these env vars.

  # Frontend Static Site (React)
  - type: web
    name: inferencemd-frontend
    runtime: static
    # Ensure frontend build command still works relative to repo root
    buildCommand: "cd frontend && npm ci && npm run build"
    staticPublishPath: ./frontend/build
    autoDeploy: true
    envVars:
      - key: CI
        value: "false"
      - key: NODE_VERSION
        value: "18"
      - key: REACT_APP_API_BASE_URL
        fromService:
          type: web
          name: inferencemd-backend
          property: host
    routes:
      - type: rewrite
        source: /*
        destination: /index.html

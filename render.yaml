# Render Deployment Configuration for InferenceMD
# Defines the services needed to run the application on Render.
# Documentation: https://render.com/docs/blueprint-spec

databases:
  # Database service for the FastAPI backend
  - name: inferencemd-db
    # region: frankfurt # Optional: Choose a region close to you/your users
    plan: free # Or choose a paid plan for production needs
    postgresMajorVersion: 14 # Specify PostgreSQL version
    ipAllowList: [] # Allows connections from Render services by default

services:
  # Backend Web Service (FastAPI)
  # Runs the Python backend application using Docker.
  - type: web
    name: inferencemd-backend
    # Using Docker runtime
    runtime: docker
    plan: free
    # Docker specific settings
    dockerfilePath: ./backend/Dockerfile # Path to the Dockerfile relative to repo root
    dockerContext: . # Build context directory is the repository root
    # buildCommand and startCommand are now handled by the Dockerfile's build process and CMD/ENTRYPOINT
    healthCheckPath: / # Use the root path for health checks. Ensure the container exposes the app on $PORT and this path returns 2xx without auth.
    envVars:
      # Database and Core Settings
      - key: DATABASE_URL
        fromDatabase:
          name: inferencemd-db
          property: connectionString
      - key: SECRET_KEY
        generateValue: true
      # PYTHON_VERSION is handled by pythonVersion field above
      # PYTHONPATH is not typically needed when running from the correct directory
      # CORS_ORIGINS will be constructed in config.py using RENDER_FRONTEND_URL
      - key: RENDER_FRONTEND_URL # Variable to hold the raw URL from Render
        fromService:
          type: web
          name: inferencemd-frontend
          property: url # Get the full URL string
      
      # LLM Provider Configuration
      # Set this to one of: "azure", "openai", "gemini", "deepseek"
      - key: LLM_PROVIDER
        value: "gemini"
      
      # Google Gemini Settings (when LLM_PROVIDER="gemini")
      # IMPORTANT: Store sensitive keys like API keys as Render Secret Files or in Environment Groups
      # See: https://render.com/docs/configure-environment-variables#secret-files
      # Example using a secret key named 'google_api_key':
      - key: GOOGLE_API_KEY
        fromSecretKeyRef:
          name: google_api_key # Name of the secret key you create in Render dashboard
          key: GOOGLE_API_KEY # The specific key within the secret (if using env group)
      - key: LLM_MODEL_NAME
        value: gemini-2.5-pro-preview-03-25 # Example: Use a specific model version
      
      # Common LLM Parameters (apply to all providers)
      - key: LLM_TEMPERATURE
        value: "0.5"
      - key: LLM_MAX_TOKENS
        value: "4096"
      - key: LLM_TIMEOUT
        value: "120"
      - key: LLM_MAX_RETRIES
        value: "2"
      
      # File Storage Settings - Removed, defaults in config.py will be used
      # (e.g., 'reports', 'notes', 'static' relative to backend dir)
      # If persistent storage is needed, add a Render Disk and set these env vars.

  # Frontend Static Site (React)
  - type: web
    name: inferencemd-frontend
    runtime: static
    # Ensure frontend build command still works relative to repo root
    buildCommand: "cd frontend && npm ci && npm run build"
    staticPublishPath: ./frontend/build
    envVars:
      - key: CI
        value: "false"
      - key: NODE_VERSION
        value: "18"
      - key: REACT_APP_API_BASE_URL
        fromService:
          type: web
          name: inferencemd-backend
          property: host
    routes:
      - type: rewrite
        source: /*
        destination: /index.html
